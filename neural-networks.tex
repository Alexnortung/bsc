\section{Neural networks}%
\label{sec:Neural networks}

A neural network is a machine learning model, that consists of a set of layers. Neural networks are used to recognize patterns in data and be able to classify it. The way it works is by having some input data that can be used as a input to the network, and then be propageted through each layer. In an untrained network the output would not make much sense, but by training the network with known input and expected output data, a network can give quite accurate results.

\subsection{Fully connected layer}%
\label{sub:Fully connected layer}

The fully connected layer is also known as a dense layer or a linear layer.
This layer can be represented as a vector of inputs $\bm{x}$, where each input has a "connection" to each output. Where the output can also be represented as a vector $\bm{y}$
In [TODO: INSERT FIGURE] it is illustrated what is the input, a connection and the output.

Each connection has a weight which is multiplied to the value from the input.
The connections can be represented as a matrix $\bm{W}$, which will now be referred to as the weights.
The weight $w_{ij}$ corresponds to the weight between input $i$ and output $j$.

When the data is propagated through this layer, it goes from the input through a connection, where it is multiplied by the weight and then each value that ends in that output is summed together.

$$y''_j = \sum_{i=0}^D w_{ji}x_i  $$

Once the data has reached the output, there should also be added a bias, which can make the layer be able to fit better to the desired result.
The bias $\bm{b}$ can be represented as a vector with the same length as the output $M$.

$$y'_j = b_j + \sum_{i=0}^D w_{ji}x_i$$

The vector $\bm{y'}$ can thus be represented as a vector to matrix multiplication and adding the bias vector

$$\bm{y'} = \bm{b} + \bm{W}\bm{x}$$

At last there should be applied an activation function, this will be denoted $\sigma$.
So $y_j$ becomes

$$y_j = \sigma \left(y'_j\right)$$

\subsection{Activation functions}%
\label{sub:Activation functions}

Activation functions have an important role in neural networks, since they can introduce non-linearity. This is important when training the network.
Some common activation functions can be seen in \autoref{tab:activation}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|}\hline
\textbf{Function} & $\bm{\sigma}$ \\\hline
tanh     & $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ \\\hline
ReLU     & $max(0, x)$ \\\hline
sigmoid  & $\frac{1}{1+e^{-x}}$ \\\hline
softmax  & $\frac{e^x}{\sum^K_{k=1} e^{z_k}}$ \\\hline
\end{tabular}
\caption{A list of common activation functions.}
\label{tab:activation}
\end{table}

\subsection{Multilayer perceptron}%
\label{sub:Multilayer perceptron}

A multiplayer perceptron is a neural network consisting of only fully connected layers.

\subsection{Convolutional layer}

A neural network with one or more convolutional layers are called a convolutional neural network.
The convolutional layer is often takes a three dimensional input, where the two dimensions can be an image and the third dimension is the channels. The channels of an image is most often the different color channels such as red green and blue.

The convolutional layer works by having a kernel that convolves through the input. The kernel should not be confused with kernels in cuda, it is also known as a filter. The kernel has the same dimensions as the input, but the sizes of the image dimensions of the kernel should be less than or equal to the image dimensions of the input. This can be seen on \autoref{fig:conv_input_and_kernel}.

\begin{figure}
    \centering
    \hfill
    \subfloat[][An example for a $3 \times 3$ input to a convolutional layer]{$\begin{bmatrix}
        X_{11} & X_{12} & X_{13} \\
        X_{21} & X_{22} & X_{23} \\
        X_{31} & X_{32} & X_{33} 
    \end{bmatrix}$}
    \hfill
    \subfloat[][An example for a $2\times 2$ kernel]{$\begin{bmatrix}
        K_{11} & K_{12} \\
        K_{21} & K_{22}
    \end{bmatrix}$}
    \hfill
    \null
    \caption{An example for an input and a kernel to be used in a convolutional layer}
    \label{fig:conv_input_and_kernel}
\end{figure}


The convolutional operation works by taking the dotproduct of the kernel with a slice of the input, then adding a bias and applying the activation function. This is done for each channel with the same slice. These values are then stored in the output. This is repeated for a new slice until the the end of the input. The operaion is visualized in [TODO: INSERT FIGURE].

[INSERT FIGURE]

To put it as a formula for the 2 dimensional image, let $\bm{X}$ be the input, $\bm{Y}$ the output, $C$ the number of channels, $\bm{K}$ the kernel, $k_x$ and $k_y$ the size of the kernel and $s_x$ and $s_y$ be the stride which the kernel is moving through the image.

$$Y_{ijc} = \sum^{i + k_x}_{i_2 = i \cdot s_x} \sum^{j + k_y}_{j_2 = j \cdot s_y} X_{i_2j_2c} K_{i_2j_2c} $$

Where $i$ is the index in the first image dimension, $j$ is the index in the second image dimension and $c$ is the current chnanel.

\subsection{Maxpooling layer}

\subsection{Netowork training}



\subsection{Network training}
