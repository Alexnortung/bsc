\section{Neural networks}%
\label{sec:Neural networks}

A neural network is a machine learning model, that consists of a set of layers. Neural networks are used to recognize patterns in data and be able to classify it. The way it works is by having some input data that can be used as a input to the network, and then be propageted through each layer. In an untrained network the output would not make much sense, but by training the network with known input and expected output data, a network can give quite accurate results.

\subsection{Fully connected layer}%
\label{sub:Fully connected layer}

The fully connected layer is also known as a dense layer or a linear layer.
This layer can be represented as a vector of inputs $\bm{x}$, where each input has a "connection" to each output. Where the output can also be represented as a vector $\bm{y}$
In [TODO: INSERT FIGURE] it is illustrated what is the input, a connection and the output.

Each connection has a weight which is multiplied to the value from the input.
The connections can be represented as a matrix $\bm{W}$, which will now be referred to as the weights.
The weight $w_{ij}$ corresponds to the weight between input $i$ and output $j$.

When the data is propagated through this layer, it goes from the input through a connection, where it is multiplied by the weight and then each value that ends in that output is summed together.

$$y''_j = \sum_{i=0}^D w_{ji}x_i  $$

Once the data has reached the output, there should also be added a bias, which can make the layer be able to fit better to the desired result.
The bias $\bm{b}$ can be represented as a vector with the same length as the output $M$.

$$y'_j = b_j + \sum_{i=0}^D w_{ji}x_i$$

The vector $\bm{y'}$ can thus be represented as a vector to matrix multiplication and adding the bias vector

$$\bm{y'} = \bm{b} + \bm{W}\bm{x}$$

At last there should be applied an activation function, this will be denoted $\sigma$.
So $y_j$ becomes

$$y_j = \sigma \left(y'_j\right)$$

\subsection{Activation functions}%
\label{sub:Activation functions}

Activation functions have an important role in neural networks, since they can introduce non-linearity. This is important when training the network.
Some common activation functions can be seen in \autoref{tab:activation}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|}\hline
\textbf{Function} & $\bm{\sigma}$ \\\hline
tanh     & $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ \\\hline
ReLU     & $max(0, x)$ \\\hline
sigmoid  & $\frac{1}{1+e^{-x}}$ \\\hline
softmax  & $\frac{e^x}{\sum^K_{k=1} e^{z_k}}$ \\\hline
\end{tabular}
\caption{A list of common activation functions.}
\label{tab:activation}
\end{table}

\subsection{Convolutional layer}

A neural network with one or more convolutional layers are called a convolutional neural network.
The convolutional layer is often takes a three dimensional input, where the two dimensions can be an image and the third dimension is the channels. The channels of an image is most often the different color channels such as red, green and blue.

The goal of a convolutional layer is to detect certain features, in the given input. The network will be able to detect more and more complex features depending on how many convolutional layers the network has.

The convolutional layer works by having a kernel $\bm{K}$ that convolves through the input. The kernel should not be confused with kernels in cuda, it is also known as a filter. The kernel has the same dimensions as the input, but the sizes of the image dimensions of the kernel should be less than or equal to the image dimensions of the input. This can be seen on \autoref{fig:conv_input_and_kernel}.

\begin{figure}
    \centering
    \hfill
    \subfloat[][An example for a $3 \times 3$ input to a convolutional layer]{$\begin{bmatrix}
        X_{11} & X_{12} & X_{13} \\
        X_{21} & X_{22} & X_{23} \\
        X_{31} & X_{32} & X_{33} 
    \end{bmatrix}$}
    \hfill
    \subfloat[][An example for a $2\times 2$ kernel]{$\begin{bmatrix}
        K_{11} & K_{12} \\
        K_{21} & K_{22}
    \end{bmatrix}$}
    \hfill
    \null
    \caption{An example for an input and a kernel to be used in a convolutional layer}
    \label{fig:conv_input_and_kernel}
\end{figure}


The convolutional operation works by taking the dotproduct of the kernel with a slice of the input, then adding a bias and applying the activation function. This is done for each channel with the same slice. These values are then stored in the output. This is repeated for a new slice until the the end of the input. The operaion is visualized in \autoref{fig:conv_operation}.

\begin{figure}
    \centering
    $$\begin{bmatrix}
        \color{red} X_{11} & \color{red} X_{12} & X_{13} \\
        \color{red} X_{21} & \color{red} X_{22} & X_{23} \\
        X_{31} & X_{32} & X_{33} 
    \end{bmatrix} \Rightarrow \begin{bmatrix}
        \color{red} X_{11} K_{11} + X_{12} K_{12} + X_{21} K_{21} + X_{22} K_{22} & \\
         &
    \end{bmatrix}$$\\
    $$\begin{bmatrix}
        X_{11} & \color{red} X_{12} & \color{red} X_{13} \\
        X_{21} & \color{red} X_{22} & \color{red} X_{23} \\
        X_{31} & X_{32} & X_{33} 
    \end{bmatrix} \Rightarrow \begin{bmatrix}
         Y''_{11} &\color{red} X_{12} K_{12} + X_{13} K_{13} + X_{22} K_{22} + X_{23} K_{23} \\
         &
    \end{bmatrix}$$\\
    $$\begin{bmatrix}
        X_{11} & X_{12} & X_{13} \\
        \color{red} X_{21} & \color{red} X_{22} & X_{23} \\
        \color{red} X_{31} & \color{red} X_{32} & X_{33}
    \end{bmatrix} \Rightarrow \begin{bmatrix}
        Y''_{11} & Y''_{12} \\
        \color{red} X_{21} K_{21} + X_{22} K_{22} + X_{31} K_{31} + X_{32} K_{32} & 
    \end{bmatrix}$$\\
    $$\begin{bmatrix}
        X_{11} & X_{12} & X_{13} \\
        X_{21} & \color{red} X_{22} & \color{red} X_{23} \\
        X_{31} & \color{red} X_{32} & \color{red} X_{33}
    \end{bmatrix} \Rightarrow \begin{bmatrix}
         Y''_{11} & Y''_{12} \\
         Y''_{21} & \color{red} X_{22} K_{22} + X_{23} K_{23} + X_{32} K_{32} + X_{33} K_{33}
    \end{bmatrix}$$\\
    \caption{A visualization of the convolutional layer operation. The red numbers on the left are the input values that are currently in the window. The red number on the right is the result of the dotproduct of the window and the kernel.}
    \label{fig:conv_operation}
\end{figure}

To put it as a formula for the 2 dimensional image,
let $\bm{X}$ be the input,
$\bm{Y}$ be the output.
$\bm{b}$ be a vector of the bias with the length equal to the number of output channels.
$C_{in}$ the number of input channels.
$\bm{K}$ the kernel.
$k_x$ and $k_y$ the size of the kernel and $s_x$ and $s_y$ be the stride which the kernel is moving through the image.

$$Y_{ijc_{out_l}} = b_{c_{out_l}} + \sum^{C_{in}}_{c = 1} \sum^{i \cdot s_x + k_x}_{i_2 = i \cdot s_x} \sum^{j \cdot s_y + k_y}_{j_2 = j \cdot s_y} X_{i_2j_2c} K_{i_2j_2c} $$

Where $i$ is the index in the first image dimension, $j$ is the index in the second image dimension and $c_{out_l}$ is the current output chnanel.

$i$, $j$ and $l$ are in the following ranges

$$i = 1, ..., \frac{n_x - k_x}{s_x} + 1$$
$$j = 1, ..., \frac{n_y - k_y}{s_y} + 1$$
$$l = 1, ..., C_{out}$$

Where $n_x$ and $n_y$ are the image dimensions.

This means the dimensions of $\bm{Y}$ is

$$\left(C_{out}\right) \times \left(\frac{n_x - k_x}{s_x} + 1\right) \times \left(\frac{n_y - k_y}{s_y} + 1\right) $$

\subsection{Maxpooling layer}

The max pooling layer splits the input into different windows, and outputs the maximum value in each window. The operation is more easily understood in \autoref{fig:max_pool}.

\begin{figure}[htpb]
    \centering
    $$\begin{bmatrix}
        \color{red}1 & \color{red} 2 & \color{green} 3 & \color{green} 4 \\
        \color{red}8 & \color{red}9 & \color{green}10 & \color{green}11 \\
        \color{blue}7 & \color{blue}1 & \color{magenta}2 & \color{magenta}6 \\
        \color{blue}2 & \color{blue}1 & \color{magenta}9 & \color{magenta}3
        \end{bmatrix} \Rightarrow \begin{bmatrix}
        \color{red}9 & \color{green}11 \\
        \color{blue}7 & \color{magenta}9
        \end{bmatrix}$$
    \caption{Illustration of a max pooling operation on a $4\times 4$ matrix with a $2\times 2$ window. The window will process the numbers of the same color and output the same color.}
    \label{fig:max_pool}
\end{figure}

In this example the first slice will be $[1, 2, 8, 9]$, the slice is flattened for easier readability. Then the greatest number is $9$ which will be set in the output. This is repeated for the next slice and then repeated until the window reaches the end of the input.

\subsection{Loss functions}%
\label{sub:Loss functions}

To determine how close the network is at predicting something, a loss function can be used.
The loss function works by taking the output of the network and comparing it with the expected value or label.

There are different loss functions that finds the loss in different ways. Some common loss functions can be seen in [INSERT TABLE]

\subsection{Network training}

The goal of using a neural network is to make it correctly classify some input data. This can be done by training the network.
The network can be trained by "optimizing" the weights for each layer. Here optimizing is referred to as giving a better result and not as code optimizing.
One method of optimizing the network is by using gradient descent.
The function $E$ which the gradient will be found, is the forward propagation of the network as a function of the weights, composed with a loss function, which is shown in the following formula.

$$E = L(f(\bm{W}_1, \bm{W}_2, ..., \bm{W}_n))$$

Where $n$ is the number of layers in the network, $\bm{W}_i$ is the weights for layer $i$ where $i = 1...n$, $L$ is a loss function and $f$ is the forward propagation.
Gradient descent works because the loss function will be minimized, thus the weights need to change to fit the input and label data.


